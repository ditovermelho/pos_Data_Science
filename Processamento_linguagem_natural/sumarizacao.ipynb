{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-5.2.2-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Downloading lxml-5.2.2-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB 682.7 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.1/3.8 MB 1.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.7/3.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.4/3.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 17.5 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-5.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento de língua natural (PLN) é uma subárea da ciência da computação, inteligência artificial e da linguística que estuda os problemas da geração e compreensão automática de línguas humanas naturais. Sistemas de geração de língua natural convertem informação de bancos de dados de computadores em linguagem compreensível ao ser humano e sistemas de compreensão de língua natural convertem ocorrências de linguagem humana em representações mais formais, mais facilmente manipuláveis por programas de computador. Alguns desafios do PLN são compreensão de língua natural, fazer com que computadores extraiam sentido de linguagem humana ou natural e geração de língua natural.\n",
      "A história do PLN começou na década de 1950, quando Alan Turing publicou o artigo \"Computing Machinery and Intelligence\", que propunha o que agora é chamado de teste de Turing como critério de inteligência.\n",
      "Em 1954, a experiência de Georgetown envolveu a tradução automática de mais de sessenta frases russas para o inglês. Os autores afirmaram que dentro de três ou cinco anos a tradução automática seria um problema resolvido.[2] No entanto, os avanços reais foram muito mais lentos do que o previsto e, após o relatório ALPAC em 1966, que constatou que a pesquisa de dez anos não conseguiu satisfazer as expectativas, o financiamento para este estudo em tradução automática foi reduzido drasticamente. Poucas pesquisas em tradução automática foram conduzidas até o final dos anos 80, quando os primeiros sistemas estatísticos de tradução foram desenvolvidos.\n",
      "Alguns sistemas de PLN bem sucedidos desenvolvidos nos anos 60 foram SHRDLU, um sistema de língua natural que trabalhava em \"blocks worlds\" com vocabulário restrito e ELIZA, uma simulação de um psicoterapeuta escrita por Joseph Weizenbaum entre 1964 e 1966. Usando pouca informação sobre o pensamento ou a emoção humana, ELIZA criava, em alguns casos, interações surpreendentemente humanas. Quando o \"paciente\" excedia a base de conhecimento do programa, ELIZA fornecia uma resposta genérica, por exemplo, respondendo a \"Minha cabeça dói\" com \"Por que você diz que sua cabeça dói?\".\n",
      "Durante a década de 1970, muitos programadores começaram a escrever \"ontologias conceituais\", que estruturaram a informação do mundo real em dados compreensíveis por computadores. Exemplos são MARGIE (SCHANK, 1975), SAM (CULLINGFORD, 1978), PAM (WILENSKY, 1978), TaleSpin (MEEHAN, 1976), QUALM (LEHNERT, 1977), Politics (CARBONELL, 1979) e Plot Units (LEHNERT, 1981 ). Neste período, muitos chatterbots foram escritos, como PARRY, Racter e Jabberwacky.\n",
      "Até a década de 1980, a maioria dos sistemas de PLN se baseava em conjuntos complexos de regras manuscritas. A partir do final dos anos 1980, no entanto, houve uma revolução no PLN com a introdução de algoritmos de aprendizagem automática (aprendizado de máquina) para o processamento de linguagem. Isto foi devido tanto ao aumento constante do poder computacional (ver Lei de Moore) quanto à diminuição gradual da dominância das teorias da linguística chomskyanas (como a gramática gerativa), cujos fundamentos teóricos desestimularam o tipo de corpus linguístico que está subjacente à abordagem da aprendizagem automática ao processamento da linguagem[3].\n",
      "Alguns dos algoritmos de aprendizado de máquinas mais antigos, como as árvores de decisão, produziam sistemas de regras rígidas então semelhantes às regras existentes na escritas à mão. No entanto, a marcação de partes da fala (part-of-speech tagging) introduziu o uso de modelos ocultos de Markov para o PLN e, cada vez mais, a pesquisa se concentrava em modelos estatísticos, que tomam decisões suaves e probabilísticas baseadas na atribuição de pesos reais aos recursos que compõem dados de entrada. Os modelos de linguagem de cache, sobre os quais muitos sistemas de reconhecimento de fala agora dependem, são exemplos de tais modelos estatísticos. Esses modelos são geralmente mais robustos quando dados informações desconhecidas, especialmente entrada que contém erros (como é muito comum para dados do mundo real) e produzem resultados mais confiáveis quando integrados em sistemas maiores que compreendem múltiplas tarefas.\n",
      "Muitos dos sucessos iniciais notáveis ocorreram no campo da tradução automática, devido especialmente ao trabalho de pesquisa da IBM, que desenvolveu modelos estatísticos mais elaborados. Estes sistemas foram capazes de tirar proveito de corpora textuais multilíngues existentes  produzidos pelo Parlamento do Canadá e a União Europeia como resultado de leis que exigem a tradução de todos os processos governamentais em todas as línguas oficiais dos países. No entanto, a maioria dos sistemas dependia de corpora desenvolvido especificamente para tarefas implementadas por esses sistemas, o que era (e muitas vezes continua sendo) uma grande limitação no sucesso dos mesmo. Como resultado, uma grande quantidade de pesquisa passou de quantidades de dados limitadas a métodos de aprendizagem mais eficazes.\n",
      "Pesquisas recentes têm se concentrado cada vez mais em algoritmos de aprendizagem semi-supervisionados e sem supervisão. Esses algoritmos são capazes de aprender com dados que não foram anotados manualmente com as respostas desejadas ou usando uma combinação de dados anotados e não anotados. Geralmente, esta tarefa é muito mais trabalhosa do que a aprendizagem supervisionada e normalmente produz resultados menos precisos para uma quantidade específica de dados de entrada. No entanto, há uma enorme quantidade de dados não anotados disponíveis (incluindo, entre outras coisas, todo o conteúdo da World Wide Web), que muitas vezes pode compensar os resultados inferiores.\n",
      "Os algoritmos modernos de PLN baseiam-se na aprendizagem mecânica, especialmente na aprendizagem de máquinas estatísticas. O paradigma da aprendizagem mecânica é diferente do da maioria das tentativas anteriores de processamento da linguagem. Anteriormente, implementações de tarefas de processamento de linguagem envolviam a codificação direta de grandes conjuntos de regras. O paradigma da aprendizagem automática (ou aprendizagem automática) induz a aprendizagem automática de regras através de análises de corpora de exemplos típicos do mundo real ao invés de usar algoritmos gerais de aprendizagem (muitas vezes, embora nem sempre, baseados em inferência estatística). Um corpus (plural \"corpora\") é um conjunto de documentos (ou frases individuais) que foram anotados à mão com os valores corretos a serem aprendidos.\n",
      "Muitas classes diferentes de algoritmos de aprendizado de máquina foram aplicadas a tarefas de PLN. Esses algoritmos tomam como entrada um grande conjunto de \"recursos\" que são gerados a partir de dados de entrada.\n",
      "Alguns dos algoritmos mais usados, como árvores de decisão, produziam sistemas de regras rígidas semelhantes aos sistemas de regras manuscritas mais comuns. No entanto, cada vez mais, a pesquisa tem se concentrado em modelos estatísticos, que tomam decisões flexíveis e probabilísticas baseadas em agregar pesos reais a cada característica de entrada. Tais modelos têm a vantagem de poder expressar a certeza relativa de muitas respostas possíveis diferentes em vez de apenas uma, produzindo resultados mais confiáveis quando esse modelo é incluído como um componente de um sistema maior.\n",
      "Os sistemas baseados em algoritmos de aprendizagem mecânica têm muitas vantagens em relação às regras produzidas manualmente:\n",
      "O subcampo de PLN dedicado a abordagens de aprendizagem é conhecido como aprendizagem de língua natural (NLL) e sua conferência, a CoNLL,[4] e orgão central, o SIGNLL,[5] são patrocinados pela ACL, reconhecendo também as suas ligações com linguística computacional e aquisição de linguagem. Quando o objetivo da pesquisa de aprendizagem de linguagem computacional é entender mais sobre aquisição de linguagem humana, ou psicolinguística, a NLL sobrepõe-se no campo relacionado de psicolinguística computacional.\n",
      "A listagem a seguir traz alguns dos trabalhos mais pesquisadas em PLN. Note que alguns deles têm aplicações no mundo real, enquanto outras servem mais frequentemente como tarefas secundárias que são usadas para auxiliar na resolução de tarefas maiores. O que distingue essas tarefas de outras tarefas potenciais e reais de PLN não é apenas o volume de pesquisa dedicado a elas, mas o fato de que para cada uma há tipicamente uma definição de problema bem especificada, uma métrica padrão para avaliar a tarefa, corpora padrão em que a tarefa pode ser avaliada e as competições dedicadas à tarefa específica.\n",
      "Sumarização automática\n",
      "Resolução de correferência\n",
      "Análise do Discurso\n",
      "Maquina de tradução\n",
      "Segmentação morfológica\n",
      "Reconhecimento de entidade nomeada (NER)\n",
      "Geração de língua natural\n",
      "Compreensão da língua natural\n",
      "Reconhecimento óptico de caracteres (OCR)\n",
      "Marcação de classe gramatical\n",
      "Análise sintática (Parsing)\n",
      "Respostas a perguntas\n",
      "Extração de relacionamento\n",
      "Quebra de frases (sentence boundary disambiguation)\n",
      "Análise de subjetividade (sentiment analysis ou opinion mining)\n",
      "Reconhecimento de fala\n",
      "Segmentação de fala\n",
      "Análise morfológica e reconhecimento de tópicos\n",
      "Análise morfológica e segmentação de palavras\n",
      "Desambiguação\n",
      "Recuperação de informação (IR)\n",
      "Extração de informação (IE)\n",
      "Avaliação intrínseca vs. extrínseca\n",
      "A avaliação intrínseca considera um sistema PNL isolado e caracteriza seu desempenho em relação a um resultado padrão-excelência, conforme definido pelos avaliadores. A avaliação extrínseca, também chamada de avaliação em uso, considera o sistema PLN em um cenário mais complexo como um sistema embutido ou uma função precisa para um usuário humano. O desempenho extrínseco do sistema é então caracterizado em termos de utilidade em relação à tarefa global do sistema estranho ou do utilizador humano. Por exemplo, considere um analisador sintático que é baseado na saída de alguma parte do tagger de fala (POS). Uma avaliação intrínseca executaria o marcador POS em dados estruturados e compararia a saída do sistema do marcador POS com a saída padrão ouro. Uma avaliação extrínseca executaria o analisador com algum outro marcador POS e, em seguida, com o marcador POS novo e compara a precisão de análise.\n",
      "Caixa preta vs. Avaliação da caixa de vidro\n",
      "A avaliação em caixa preta requer que alguém execute um sistema PLN em um conjunto de dados de amostra e para medir uma série de parâmetros relacionados com a qualidade do processo, como velocidade, confiabilidade, consumo de recursos e, principalmente, a qualidade do resultado, como a precisão da anotação de dados ou a fidelidade de uma tradução. A avaliação da caixa de vidro examina a concepção do sistema; Os algoritmos que são implementados, os recursos linguísticos que utiliza, como o tamanho do vocabulário ou a expressão definida de cardinalidade. Dada a complexidade dos problemas da PLN, muitas vezes é difícil prever o desempenho apenas com base na avaliação da caixa de vidro; Mas este tipo de avaliação é mais informativo no que diz respeito à análise de erros ou desenvolvimentos futuros de um sistema.\n",
      "Automática vs. avaliação manual\n",
      "Em muitos casos, procedimentos automáticos podem ser definidos para avaliar um sistema de PLN, comparando sua saída com o padrão de excelência. Embora o custo de reproduzir o padrão de excelência possa ser bastante elevado, avaliação automática de bootstrapping sobre os mesmos dados de entrada pode ser repetida quantas vezes for necessário sem custos adicionais desordenados. No entanto, para muitos problemas de PLN a definição precisa de um padrão de excelência é uma tarefa complexa e pode se revelar impossível quando o acordo inter-anotador é insuficiente. A avaliação manual é melhor realizada por juízes humanos instruídos para estimar a qualidade de um sistema, ou mais frequentemente de uma amostra de sua produção, com base em uma série de critérios. Embora, graças à sua competência linguística, os juízes humanos possam ser considerados como a referência para uma série de tarefas de processamento de linguagem, há também uma variação considerável em suas classificações. É por isso que a avaliação automática é, por vezes, referida como avaliação objetiva enquanto a avaliação humana é perspectiva.\n",
      "Um subcomitê ISO está trabalhando para facilitar a interoperabilidade entre recursos lexicais e programas PLN. O subcomitê faz parte do ISO / TC37 e é chamado ISO / TC37 / SC4. Alguns padrões ISO já estão publicados, mas a maioria deles está em construção, principalmente na representação de léxico (ver LMF), anotação e registro de categoria de dados.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagina = urllib.request.urlopen('https://pt.wikipedia.org/wiki/Processamento_de_linguagem_natural')\n",
    "artigo = pagina.read()\n",
    "\n",
    "artigo_convertido = bs.BeautifulSoup(artigo,'lxml')\n",
    "\n",
    "paragrafos = artigo_convertido.find_all('p')\n",
    "\n",
    "artigo_texto = \"\"\n",
    "\n",
    "for p in paragrafos:\n",
    "    artigo_texto += p.text\n",
    "    \n",
    "print(artigo_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_texto = re.sub(r'\\[[0-9]*\\]', ' ', artigo_texto)\n",
    "artigo_texto = re.sub(r'\\s+', ' ', artigo_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_texto_formatado = re.sub('[^a-zA-Z]', ' ', artigo_texto )\n",
    "artigo_texto_formatado = re.sub(r'\\s+', ' ', artigo_texto_formatado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_sentencas = nltk.sent_tokenize(artigo_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "dic_palavra_freq = {}\n",
    "for palavra in nltk.word_tokenize(artigo_texto_formatado):\n",
    "    if palavra not in stopwords:\n",
    "        if palavra not in dic_palavra_freq.keys():\n",
    "            dic_palavra_freq[palavra] = 1\n",
    "        else:\n",
    "            dic_palavra_freq[palavra] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_palavra_frequencia_maxima = max(dic_palavra_freq.values())\n",
    "\n",
    "for palavra in dic_palavra_freq.keys():\n",
    "    dic_palavra_freq[palavra] = (dic_palavra_freq[palavra]/dic_palavra_frequencia_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenca_score = {}\n",
    "for sent in lista_sentencas:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in dic_palavra_freq.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentenca_score.keys():\n",
    "                    sentenca_score[sent] = dic_palavra_freq[palavra]\n",
    "                else:\n",
    "                    sentenca_score[sent] += dic_palavra_freq[palavra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os modelos de linguagem de cache, sobre os quais muitos sistemas de reconhecimento de fala agora dependem, são exemplos de tais modelos estatísticos. Geralmente, esta tarefa é muito mais trabalhosa do que a aprendizagem supervisionada e normalmente produz resultados menos precisos para uma quantidade específica de dados de entrada. No entanto, cada vez mais, a pesquisa tem se concentrado em modelos estatísticos, que tomam decisões flexíveis e probabilísticas baseadas em agregar pesos reais a cada característica de entrada. Muitos dos sucessos iniciais notáveis ocorreram no campo da tradução automática, devido especialmente ao trabalho de pesquisa da IBM, que desenvolveu modelos estatísticos mais elaborados. Esses algoritmos são capazes de aprender com dados que não foram anotados manualmente com as respostas desejadas ou usando uma combinação de dados anotados e não anotados. Um corpus (plural \"corpora\") é um conjunto de documentos (ou frases individuais) que foram anotados à mão com os valores corretos a serem aprendidos. Quando o objetivo da pesquisa de aprendizagem de linguagem computacional é entender mais sobre aquisição de linguagem humana, ou psicolinguística, a NLL sobrepõe-se no campo relacionado de psicolinguística computacional.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "sentenca_sumar = heapq.nlargest(7, sentenca_score, key=sentenca_score.get)\n",
    "\n",
    "sumarizacao = ' '.join(sentenca_sumar)\n",
    "print(sumarizacao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
